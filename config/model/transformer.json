{
    "model_name": "Transformer",
    "d_model": 128,
    "n_heads": 8,
    "e_layers": 1,
    "d_layers": 2,
    "d_ff": 1024,
    "factor": 3,
    "dropout": 0.01,
    "activation": "gelu",
    "output_attention": false
}